{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a71663",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 10 — LLM Training Pipeline Notebook\n",
    "\n",
    "配合课件《大语言模型解析 VII》，演示从数据准备到训练循环的核心步骤。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c93ea7",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+，建议使用支持 GPU 的环境\n",
    "- 安装 `datasets`, `transformers`, `torch`\n",
    "- 若需要下载 Hugging Face 数据集或模型，请提前配置网络或本地缓存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可按需安装依赖（默认注释，按需取消）\n",
    "# %pip install datasets transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d73d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeac475",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 · 载入 Alpaca 数据集\n",
    "Alpaca 数据集包含 instruction / input / output 字段，适合作为指令微调示例。若在离线环境，可将数据集下载至本地目录后使用 `load_dataset(\"json\", data_files=...)` 载入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c08440",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('tatsu-lab/alpaca')\n",
    "train_raw = dataset['train']\n",
    "print(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看一个原始样本\n",
    "display(train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a77be",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 · 规则过滤\n",
    "根据业务需求过滤异常样本，例如空答案、长度过短或包含敏感词的样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae943fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_example(example):\n",
    "    answer = example['output'].strip()\n",
    "    return len(answer) > 5\n",
    "\n",
    "filtered = train_raw.filter(keep_example)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ebd4c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 · 统一字段结构\n",
    "课程中推荐将样本整理为 `messages` 列表，兼容 LLaMA Factory / ChatML 等模板。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db58e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    prompt = example['instruction'].strip()\n",
    "    if example['input']:\n",
    "        prompt += \"\\n\" + example['input'].strip()\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            {'role': 'assistant', 'content': example['output'].strip()},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "structured = filtered.map(build_messages)\n",
    "structured = structured.remove_columns([col for col in structured.column_names if col != 'messages'])\n",
    "print(structured)\n",
    "display(structured[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938d325",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 · 按长度分桶并划分训练 / 验证\n",
    "通过增加 `len_bucket` 字段实现基于长度的分层抽样，保持长短上下文在不同划分中的比例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def add_length_bucket(example):\n",
    "    user_len = len(example['messages'][0]['content'].split())\n",
    "    bucket = min(user_len // 200, 4)\n",
    "    example['len_bucket'] = bucket\n",
    "    return example\n",
    "\n",
    "bucketed = structured.map(add_length_bucket)\n",
    "\n",
    "def _bucket_counts(ds):\n",
    "    return Counter(ds['len_bucket'])\n",
    "\n",
    "bucket_counts = _bucket_counts(bucketed)\n",
    "print('原始桶统计:', bucket_counts)\n",
    "\n",
    "valid_buckets = sorted(bucket for bucket, count in bucket_counts.items() if count >= 2)\n",
    "if len(valid_buckets) < len(bucket_counts):\n",
    "    dropped = {bucket: bucket_counts[bucket] for bucket in bucket_counts if bucket not in valid_buckets}\n",
    "    print('移除样本过少的桶:', dropped)\n",
    "    bucketed = bucketed.filter(lambda example: example['len_bucket'] in valid_buckets)\n",
    "    bucket_counts = _bucket_counts(bucketed)\n",
    "    valid_buckets = sorted(bucket_counts.keys())\n",
    "    print('过滤后桶统计:', bucket_counts)\n",
    "\n",
    "if len(valid_buckets) >= 2:\n",
    "    bucket_id_map = {bucket: idx for idx, bucket in enumerate(valid_buckets)}\n",
    "\n",
    "    def remap_bucket(example):\n",
    "        example['len_bucket'] = bucket_id_map[example['len_bucket']]\n",
    "        return example\n",
    "\n",
    "    bucketed = bucketed.map(remap_bucket, desc='Remap buckets')\n",
    "    bucketed = bucketed.cast_column('len_bucket', ClassLabel(num_classes=len(valid_buckets)))\n",
    "\n",
    "    split_data = bucketed.train_test_split(\n",
    "        test_size=0.02,\n",
    "        seed=42,\n",
    "        stratify_by_column='len_bucket',\n",
    "    )\n",
    "    print('使用分层抽样完成数据划分。')\n",
    "else:\n",
    "    print('可用桶数量不足，退回到随机划分。')\n",
    "    split_data = bucketed.train_test_split(\n",
    "        test_size=0.02,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "train_data = split_data['train']\n",
    "val_data = split_data['test']\n",
    "print('Train size:', len(train_data))\n",
    "print('Validation size:', len(val_data))\n",
    "\n",
    "if len(valid_buckets) >= 2:\n",
    "    print('Train bucket分布:', Counter(train_data['len_bucket']))\n",
    "    print('Val bucket分布:', Counter(val_data['len_bucket']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445783aa",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5 · Tokenizer 对齐\n",
    "加载 LLaMA 系列 tokenizer，并构造模板函数，将 `messages` 转换为单段文本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343977db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1b')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print('Tokenizer vocab size:', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PREFIX = \"<|user|>\\n\"\n",
    "PROMPT_SUFFIX = \"\\n<|assistant|>\\n\"\n",
    "RESPONSE_SUFFIX = tokenizer.eos_token\n",
    "\n",
    "def build_prompt_parts(example):\n",
    "    user = example['messages'][0]['content']\n",
    "    assistant = example['messages'][1]['content']\n",
    "    prompt_text = f\"{PROMPT_PREFIX}{user}{PROMPT_SUFFIX}\"\n",
    "    response_text = f\"{assistant}{RESPONSE_SUFFIX}\"\n",
    "    return prompt_text, response_text\n",
    "\n",
    "prompt_text, response_text = build_prompt_parts(train_data[0])\n",
    "print('Prompt preview:')\n",
    "print('\\n'.join(prompt_text.splitlines()[:4]))\n",
    "print('\\nResponse preview:')\n",
    "print('\\n'.join(response_text.splitlines()[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f577028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt_text, response_text = build_prompt_parts(example)\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    response_ids = tokenizer(response_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    max_response_len = max_length - len(prompt_ids)\n",
    "    if max_response_len <= 0:\n",
    "        input_ids = prompt_ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = [-100] * len(input_ids)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    response_ids = response_ids[:max_response_len]\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + response_ids\n",
    "\n",
    "    if len(labels) < len(input_ids):\n",
    "        labels += [-100] * (len(input_ids) - len(labels))\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "tokenized_train = train_data.map(\n",
    "    tokenize,\n",
    "    remove_columns=train_data.column_names,\n",
    ")\n",
    "\n",
    "tokenized_val = val_data.map(\n",
    "    tokenize,\n",
    "    remove_columns=val_data.column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_train)\n",
    "print(tokenized_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b1997",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6 · Collate 函数\n",
    "collate 函数负责批量对齐，并为填充位置生成 `attention_mask` 与忽略的 `labels`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2549971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_lm_collate(batch, pad_id, label_pad_id=-100):\n",
    "    input_ids = [torch.tensor(example['input_ids']) for example in batch]\n",
    "    labels = [torch.tensor(example['labels']) for example in batch]\n",
    "    attention = [torch.tensor(example['attention_mask']) for example in batch]\n",
    "    print(input_ids)\n",
    "    padded_input = pad_sequence(input_ids, batch_first=True, padding_value=pad_id)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=label_pad_id)\n",
    "    padded_attention = pad_sequence(attention, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': padded_input,\n",
    "        'labels': padded_labels,\n",
    "        'attention_mask': padded_attention,\n",
    "    }\n",
    "\n",
    "collate_preview = causal_lm_collate([\n",
    "    tokenized_train[0],\n",
    "    tokenized_train[1],\n",
    "], pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "for key, value in collate_preview.items():\n",
    "    print(key, value.shape)\n",
    "print('Label sample:', collate_preview['labels'][0][:])\n",
    "print('Label sample:', collate_preview['input_ids'][0][:])\n",
    "print('Label sample:', collate_preview['attention_mask'][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ff7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: causal_lm_collate(batch, pad_id=tokenizer.pad_token_id),\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "for key, value in batch.items():\n",
    "    print(key, value.shape)\n",
    "print('First labels row:', batch['labels'][0][:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec28c0",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7 · Trainer 集成\n",
    "为便于演示，使用 Hugging Face 的 Tiny 模型，实践时可替换为真实的 LLaMA / Qwen / Baichuan 权重。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'hf-internal-testing/tiny-random-OPTForCausalLM'\n",
    ")\n",
    "\n",
    "print('Model loaded with parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='checkpoints/lecture10-demo',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=lambda batch: causal_lm_collate(batch, pad_id=tokenizer.pad_token_id),\n",
    ")\n",
    "\n",
    "print('Trainer ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b7763",
   "metadata": {},
   "source": [
    "\n",
    "> **提示**：实际训练 LLaMA 等大模型需要显著的显存与计算资源，建议使用分布式/QLoRA/DeepSpeed 等手段；本示例仅演示数据与代码流程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d43508",
   "metadata": {},
   "source": [
    "\n",
    "## 附录 · 与 LLaMA Factory 的衔接\n",
    "- 将 `build_messages`/`tokenize` 逻辑注册为 `preprocess_func`\n",
    "- 在 `config/*.yaml` 中设置 `dataset`, `template`, `data_collator` 等字段\n",
    "- LoRA/QLoRA 配置示例见课件 `lora_qlora.yaml`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
